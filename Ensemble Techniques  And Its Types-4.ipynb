{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d403630a-5357-401f-9104-883cfb8259a8",
   "metadata": {},
   "source": [
    "Q1. You are working on a machine learning project where you have a dataset containing numerical and\n",
    "categorical features. You have identified that some of the features are highly correlated and there are\n",
    "missing values in some of the columns. You want to build a pipeline that automates the feature\n",
    "engineering process and handles the missing values.\n",
    "Ans:-To address the challenges of handling both numerical and categorical features, managing missing values, and dealing with correlated features, you can create a comprehensive machine learning pipeline. This pipeline can include preprocessing steps such as imputing missing values, encoding categorical features, handling feature correlation, and possibly feature scaling. Below is a sample outline of such a pipeline using Python and popular libraries like scikit-learn and pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35492923-faa2-4909-a892-71b3a442adcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor  # or RandomForestClassifier for classification tasks\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error  # for regression tasks\n",
    "\n",
    "# Load your dataset\n",
    "# df = pd.read_csv(\"your_dataset.csv\")\n",
    "\n",
    "# Assuming 'target_column' is your target variable\n",
    "X = df.drop('target_column', axis=1)\n",
    "y = df['target_column']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define numerical and categorical features\n",
    "numerical_features = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Create a preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', SimpleImputer(strategy='mean'), numerical_features),\n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Add a step for handling feature correlation if necessary\n",
    "# For example, you can use a VarianceThreshold or a custom transformer\n",
    "\n",
    "# Create the final pipeline with a machine learning model\n",
    "model = RandomForestRegressor()  # or RandomForestClassifier for classification tasks\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the performance\n",
    "# For regression tasks\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "# For classification tasks, use appropriate evaluation metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6208b974-1da7-48bf-9791-7c96806605ea",
   "metadata": {},
   "source": [
    "Design a pipeline that includes the follow#ng steps\"\n",
    "Use an automated feature selection method to identify the #mportant features in the datasetC\n",
    "Create a numerical pipeline that includes the follow#ng steps\"\n",
    "Impute the m#ss#ng values in the numer#cal columns using the mean of the column valuesC\n",
    "Scale the numerical columns using standardisatonC\n",
    "Create a categorical pipeline that includes the following steps\"\n",
    "Impute the m#ss#ng values in the categorical columns using the most frequent value of the columnC\n",
    "One-hot encode the categorical columnsC\n",
    "Combine the numerical and categorical pipelines using a ColumnTransformerC\n",
    "Use a Random Forest Classifier to build the final modelC\n",
    "Evaluate the accuracy of the model on the test datasetD\n",
    "Ans:-Certainly! To achieve the described pipeline with automated feature selection, numerical and categorical pipelines, and a Random Forest Classifier, you can use scikit-learn and incorporate the necessary components. Here's a sample implementation in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e57cb02-436e-4be0-a7b6-3d4947a7f891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load your dataset\n",
    "# df = pd.read_csv(\"your_dataset.csv\")\n",
    "\n",
    "# Assuming 'target_column' is your target variable\n",
    "X = df.drop('target_column', axis=1)\n",
    "y = df['target_column']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Identify numerical and categorical features\n",
    "numerical_features = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Step 1: Automated Feature Selection\n",
    "feature_selector = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "\n",
    "# Step 2: Numerical Pipeline\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Step 3: Categorical Pipeline\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Step 4: Combine Numerical and Categorical Pipelines\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline, numerical_features),\n",
    "        ('cat', categorical_pipeline, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Step 5: Final Pipeline with Feature Selection and Random Forest Classifier\n",
    "final_pipeline = Pipeline([\n",
    "    ('feature_selection', feature_selector),\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Step 6: Fit and Evaluate\n",
    "final_pipeline.fit(X_train, y_train)\n",
    "y_pred = final_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870d64ce-a8f6-4af3-bb3e-303d6d20719a",
   "metadata": {},
   "source": [
    "Q2. Build a pipeline that includes a random forest class#f#er and a logistic regression classifier, and then\n",
    "use a voting classifier to combine their predictions. Train the pipeline on the iris dataset and evaluate #ts\n",
    "accuracy.\n",
    "Ans:-\r\n",
    "Certainly! To build a pipeline that includes both a Random Forest Classifier and a Logistic Regression Classifier, and then use a Voting Classifier to combine their predictions, you can follow the steps below. We'll use the Iris dataset for this example\r",
    "code\r\n",
    "from sklearn.datasets import load_iris\r\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "from sklearn.pipeline import Pipeline\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "\r\n",
    "# Load the Iris dataset\r\n",
    "iris = load_iris()\r\n",
    "X = iris.data\r\n",
    "y = iris.target\r\n",
    "\r\n",
    "# Split the data into training and testing sets\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\r\n",
    "\r\n",
    "# Step 1: Create individual classifiers\r\n",
    "random_forest_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\r\n",
    "logistic_regression_classifier = LogisticRegression(random_state=42)\r\n",
    "\r\n",
    "# Step 2: Create a pipeline with a StandardScaler (optional)\r\n",
    "# Note: Standardizing features may improve the performance of some classifiers\r\n",
    "pipeline = Pipeline([\r\n",
    "    ('scaler', StandardScaler()),  # Optional: StandardScaler\r\n",
    "    ('ensemble', VotingClassifier(estimators=[\r\n",
    "        ('rf', random_forest_classifier),\r\n",
    "        ('lr', logistic_regression_classifier)\r\n",
    "    ], voting='hard'))\r\n",
    "])\r\n",
    "\r\n",
    "# Step 3: Fit and evaluate the pipeline\r\n",
    "pipeline.fit(X_train, y_train)\r\n",
    "y_pred = pipeline.predict(X_test)\r\n",
    "\r\n",
    "accuracy = accuracy_score(y_test, y_pred)\r\n",
    "print(f'Accuracy: {accuracy}')\r\n",
    "Explanation of the steps:\r\n",
    "\r\n",
    "Create Individual Classifiers:\r\n",
    "\r\n",
    "Create instances of the Random Forest Classifier and the Logistic Regression Classifier.\r\n",
    "Create a Pipeline:\r\n",
    "\r\n",
    "Create a pipeline that includes a StandardScaler (optional) and a Voting Classifier.\r\n",
    "The VotingClassifier combines the predictions of multiple classifiers. In this case, it uses \"hard\" voting, meaning the majority class is chosen.\r\n",
    "Fit and Evaluate:\r\n",
    "\r\n",
    "Fit the pipeline on the training data.\r\n",
    "Evaluate the accuracy of the combined classifier on the test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
